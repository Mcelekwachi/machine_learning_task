# -*- coding: utf-8 -*-
"""Machine Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/112gIV2KD6WYJxOlrw-7_jnvf4jhjwAuM
"""

!pip install catboost
!pip install lightgbm
!pip install "dask[dataframe]"
# Install GPU dependencies
!apt-get install -y libboost-all-dev
!apt-get install -y ocl-icd-libopencl1
!apt-get install -y opencl-headers
!apt-get install -y clinfo
!apt-get install -y libcompute-dev

# Verify GPU OpenCL setup
!clinfo
!pip install lightgbm --config-settings=build_ext--enable-gpu

import pandas as pd
import numpy as np
import logging
import json
import os
import shutil
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error
from sklearn.dummy import DummyRegressor
from sklearn.decomposition import TruncatedSVD
import lightgbm as lgb
from catboost import CatBoostRegressor
from google.colab import drive
import warnings
warnings.filterwarnings("ignore", message=".*dask-expr is not installed.*")
import time
from xgboost import XGBRegressor
from sklearn.ensemble import StackingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
from sklearn.linear_model import SGDRegressor, Ridge

def main():
    # Mount Google Drive
    from google.colab import drive
    drive.mount('/content/drive')

    # Define source paths in Google Drive
    drive_dir = '/content/drive/MyDrive/Colab Notebooks/dataset.zip (Unzipped Files)'
    drive_train_path = f'{drive_dir}/train.json'
    drive_test_path = f'{drive_dir}/test.json'

    # Define local paths in Colab
    local_train_path = '/content/train.json'
    local_test_path = '/content/test.json'

    # Copy files to Colab's local directory if they don't exist
    if not os.path.exists(local_train_path):
        shutil.copy(drive_train_path, local_train_path)
    if not os.path.exists(local_test_path):
        shutil.copy(drive_test_path, local_test_path)

    # Load training and testing data
    train = pd.DataFrame.from_records(json.load(open(local_train_path)))
    test = pd.DataFrame.from_records(json.load(open(local_test_path)))

    # Ensure 'id' exists in test data
    if 'id' not in test.columns:
        test['id'] = range(len(test))  # Generate dummy IDs if 'id' is missing

    # Define the label column
    label = 'n_citation'

    # Feature Engineering
    train['abstract_length'] = train['abstract'].str.len()
    test['abstract_length'] = test['abstract'].str.len()

    train['title_length'] = train['title'].str.len()
    test['title_length'] = test['title'].str.len()

    train['num_authors'] = train['authors'].str.split(',').apply(len)
    test['num_authors'] = test['authors'].str.split(',').apply(len)

    train['year_abstract_interaction'] = train['year'] * train['abstract_length']
    test['year_abstract_interaction'] = test['year'] * test['abstract_length']

    train['author_abstract_interaction'] = train['num_authors'] * train['abstract_length']
    test['author_abstract_interaction'] = test['num_authors'] * test['abstract_length']

    train['log_abstract_length'] = np.log1p(train['abstract_length'])
    test['log_abstract_length'] = np.log1p(test['abstract_length'])

    # Define the featurizer
    featurizer = ColumnTransformer(
        transformers=[
            ("year", 'passthrough', ["year"]),
            ("authors", TfidfVectorizer(analyzer='word', max_features=100), "authors"),
            ("title", TfidfVectorizer(max_features=150), "title"),
            ("abstract", TfidfVectorizer(max_features=200), "abstract"),
            ("venue", OneHotEncoder(handle_unknown='ignore'), ["venue"]),
            ("custom_features", StandardScaler(), ["log_abstract_length", "abstract_length",
                                                   "title_length", "num_authors", "year_abstract_interaction",
                                                   "author_abstract_interaction"])
        ],
        remainder='drop', sparse_threshold=0.99
    )

    # Sample a subset of data for training and validation
    train_sample = train.sample(frac=0.015, random_state=123)
    train_sample, validation_sample = train_test_split(train_sample, test_size=1/3, random_state=123)

    # Define base models with optimized hyperparameters
    random_forest = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=12)
    catboost = CatBoostRegressor(iterations=300, depth=8, learning_rate=0.05, random_state=42, verbose=0, thread_count=2)
    gradient_boosting = GradientBoostingRegressor(n_estimators=300, max_depth=6, learning_rate=0.08, random_state=42)
    xgboost = XGBRegressor(n_estimators=300, max_depth=6, learning_rate=0.05, random_state=42, n_jobs=2)
    sgd = SGDRegressor(max_iter=1000, tol=1e-3, penalty='l2', alpha=0.01, random_state=42)

    # Define stacking ensemble with Ridge as the meta-model
    base_models = [
        ('random_forest', make_pipeline(featurizer, random_forest)),
        ('catboost', make_pipeline(featurizer, catboost)),
        ('gradient_boosting', make_pipeline(featurizer, gradient_boosting)),
        ('xgboost', make_pipeline(featurizer, xgboost)),
        ('sgd', make_pipeline(featurizer, sgd))
    ]
    meta_model = Ridge(alpha=1.0)  # Slightly regularized Ridge regression
    ensemble_model = StackingRegressor(estimators=base_models, final_estimator=meta_model, n_jobs=-1)

    # Monitor system memory
    import psutil
    print(f"Available memory before training: {psutil.virtual_memory().available / 1e9:.2f} GB")

    # Train ensemble model
    print("\nTraining Optimized Stacking Ensemble model on sampled training data...")
    ensemble_model.fit(train_sample.drop([label], axis=1), np.log1p(train_sample[label].values))

    # Evaluate on validation set
    validation_pred = np.expm1(ensemble_model.predict(validation_sample.drop([label], axis=1)))  # Reverse log-transform
    validation_mae = mean_absolute_error(validation_sample[label], validation_pred)
    print(f"Optimized model validation MAE: {validation_mae:.2f}")

    # Use the ensemble model to make predictions on the full test data
    print("\nGenerating predictions for test data with the Optimized Ensemble model...")
    test_features = test.drop([label], axis=1, errors="ignore")  # Drop label if it exists
    test_predictions = np.expm1(ensemble_model.predict(test_features))  # Reverse log-transform

    # Add predictions to the test set
    test['n_citation'] = test_predictions

    # Save predictions in the required JSON format
    prediction_output = test[['id', 'n_citation']].to_dict(orient='records')
    with open('predicted.json', 'w') as f:
        json.dump(prediction_output, f, indent=2)

    print("Predictions saved to 'predicted.json'")

# Run the main function
if __name__ == "__main__":
    main()